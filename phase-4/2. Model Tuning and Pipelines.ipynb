{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8da10029",
   "metadata": {},
   "source": [
    "# Model Tuning and Ensemble Methods\n",
    "---\n",
    "## Outline\n",
    "\n",
    "1. **Introduction to Model Tuning and Pipelines**\n",
    "2. **GridSearchCV – Hyperparameter Tuning**\n",
    "3. **Ensemble Methods – Why Combine Models?**\n",
    "4. **Random Forests – The Forest is Smarter than the Tree**\n",
    "5. **Tree Ensembles – More Trees, Better Predictions**\n",
    "6. **Gradient Boosting – Turning Weakness into Strength**\n",
    "7. **XGBoost – The Power Tool for Boosting**\n",
    "8. **Mini Exercises**\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction to Model Tuning and Pipelines\n",
    "\n",
    "### What is Model Tuning?\n",
    "\n",
    "Imagine you’re cooking spaghetti. You adjust the salt, boiling time, or sauce thickness to get the best taste. Model tuning is the same - tweaking the settings (hyperparameters) of your algorithm to get the best performance.\n",
    "\n",
    "### What is a Pipeline?\n",
    "\n",
    "A pipeline is like a recipe. It organizes the steps you take in building your model:\n",
    "\n",
    "1. Preprocessing (like cleaning or scaling data)\n",
    "2. Training the model\n",
    "3. Making predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "438808f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV,train_test_split, cross_val_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ff52b9",
   "metadata": {},
   "source": [
    "**Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fd0c9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),   # Step 1: scale features\n",
    "    ('model', LogisticRegression()) # Step 2: train model\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9931d85b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. GridSearchCV – Hyperparameter Tuning\n",
    "\n",
    "### Why Tune?\n",
    "\n",
    "Different models have different \"dials\" to adjust. For example, in a Random Forest:\n",
    "\n",
    "* How many trees?\n",
    "* How deep should they grow?\n",
    "\n",
    "GridSearch tries all combinations of values and picks the best.\n",
    "\n",
    "**Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a989ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic classification data\n",
    "\n",
    "X, y = make_classification(n_samples=500, n_features=8, n_informative=5, n_classes=2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aea75564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 10, 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "# Split your data (replace X and y with your actual feature and target variables)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [3, 5, 10]\n",
    "}\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "grid = GridSearchCV(model, param_grid, cv=5)\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best Parameters:\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88d0be3",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "\n",
    "* `n_estimators`: Number of trees.\n",
    "* `max_depth`: How deep the tree can go.\n",
    "* `cv=5`: Try combinations using 5-fold cross-validation.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Ensemble Methods – Why Combine Models?\n",
    "\n",
    "Think about voting in a class. If one person is unsure, the group can still make a good decision. That’s the idea behind ensemble methods: combine several models (weak or strong) to make better predictions.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Random Forests – The Forest is Smarter than the Tree\n",
    "\n",
    "Random Forest is an ensemble of decision trees. Each tree:\n",
    "\n",
    "* Sees only part of the data\n",
    "* Grows differently\n",
    "* Votes on the final answer\n",
    "\n",
    "**Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d0cad23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.94\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, max_depth=5)\n",
    "rf.fit(X_train, y_train)\n",
    "print(\"Accuracy:\", rf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0018c9c9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Tree Ensembles – More Trees, Better Predictions\n",
    "\n",
    "Why not just grow one big tree?\n",
    "\n",
    "Because:\n",
    "\n",
    "* One tree may overfit (memorize data)\n",
    "* Multiple smaller trees reduce overfitting\n",
    "* Each tree captures different parts of the data\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Gradient Boosting – Turning Weakness into Strength\n",
    "\n",
    "Unlike Random Forests (which train trees in parallel), Gradient Boosting trains one tree at a time.\n",
    "\n",
    "Each new tree tries to correct the mistakes of the last one.\n",
    "\n",
    "**Simple Analogy**:\n",
    "\n",
    "* Student takes a test and gets some questions wrong.\n",
    "* Teacher gives a mini-quiz only on the wrong questions.\n",
    "* The student learns from mistakes and improves over time.\n",
    "\n",
    "**Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fd73459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.94\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gb.fit(X_train, y_train)\n",
    "print(\"Accuracy:\", gb.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3bc4b5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. XGBoost – The Power Tool for Boosting\n",
    "\n",
    "XGBoost is short for *Extreme Gradient Boosting*. It’s faster and often more accurate than basic boosting models.\n",
    "\n",
    "Why it’s popular:\n",
    "\n",
    "* Efficient memory use\n",
    "* Regularization to prevent overfitting\n",
    "* Handles missing data\n",
    "\n",
    "**Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0e34410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.91\n"
     ]
    }
   ],
   "source": [
    "xgb_model = xgb.XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.1)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "print(\"Accuracy:\", xgb_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9448a2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Visual Comparison\n",
    "\n",
    "Compare different models side by side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bad62a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest Avg Score: 0.92\n",
      "GradientBoosting Avg Score: 0.91\n",
      "XGBoost Avg Score: 0.91\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    'RandomForest': RandomForestClassifier(),\n",
    "    'GradientBoosting': GradientBoostingClassifier(),\n",
    "    'XGBoost': xgb.XGBClassifier()\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X, y, cv=5)\n",
    "    print(f\"{name} Avg Score: {scores.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d995db75",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Create a pipeline that scales features and fits a logistic regression.\n",
    "2. Use `GridSearchCV` to tune a Random Forest on any dataset (e.g., Titanic).\n",
    "3. Compare accuracy of RandomForest, GradientBoosting, and XGBoost on the same data.\n",
    "4. Plot feature importances from Random Forest or XGBoost to see what variables matter most.\n",
    "5. Try reducing overfitting in a boosted model by adjusting learning rate and tree depth.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
