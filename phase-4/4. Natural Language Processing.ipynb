{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0d13b5d4",
      "metadata": {},
      "source": [
        "# Natural Language Processing (NLP)\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Introduction to NLP\n",
        "\n",
        "### What is NLP?\n",
        "\n",
        "Natural Language Processing (NLP) is how computers understand and process human language. Think of it as teaching machines to understand us - how we talk, text, and write.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "* You ask Siri, “What’s the weather today?” Siri understands your sentence and gives a response - that's NLP in action.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Word Vectorization\n",
        "\n",
        "Machines don’t understand words; they understand **numbers**. So we need to **convert text to numbers**.\n",
        "\n",
        "### Techniques:\n",
        "\n",
        "#### a. Bag of Words (BoW)\n",
        "\n",
        "We count how often each word appears."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "id": "8e4baed1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "id": "67d702b9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['fun' 'is' 'love' 'nlp']\n",
            "[[0 0 1 1]\n",
            " [1 1 0 1]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = [\"I love NLP\", \"NLP is fun\"]\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(vectorizer.get_feature_names_out())  # ['fun', 'is', 'love', 'nlp']\n",
        "print(X.toarray())                        # [[0 0 1 1], [1 1 0 1]] "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a15c1fd4",
      "metadata": {},
      "source": [
        "* `I love NLP` → has 'love' and 'nlp'\n",
        "* `NLP is fun` → has 'fun', 'is', 'nlp'\n",
        "\n",
        "Each row is a sentence. Each column is a word.\n",
        "\n",
        "#### b. TF-IDF (Term Frequency – Inverse Document Frequency)\n",
        "\n",
        "This improves BoW by reducing the weight of very common words like \"the\" or \"is\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "id": "fd10b2af",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.         0.         0.81480247 0.57973867]\n",
            " [0.6316672  0.6316672  0.         0.44943642]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "X_tfidf = tfidf.fit_transform(corpus)\n",
        "print(X_tfidf.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbdedd5a",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 3. Introduction to NLP with NLTK\n",
        "\n",
        "NLTK is the most popular library for basic NLP tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "id": "c711cec6",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to Data\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to Data\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 209,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "# Choose where to store the data (this can be anywhere on your system)\n",
        "nltk_data_dir = r\"Data\\nltk_data\"\n",
        "\n",
        "nltk.download('punkt', download_dir=nltk_data_dir)\n",
        "nltk.download('punkt_tab', download_dir=nltk_data_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "id": "31034cc2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Natural', 'language', 'processing', 'is', 'amazing', '!']\n"
          ]
        }
      ],
      "source": [
        "nltk.data.path.append(r\"Data\\nltk_data\")\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"Natural language processing is amazing!\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eedbde4f",
      "metadata": {},
      "source": [
        "**Output**: `['Natural', 'language', 'processing', 'is', 'amazing', '!']`\n",
        "\n",
        "This is called **tokenization** - breaking text into words.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Introduction to Regular Expressions (Regex)\n",
        "\n",
        "Regular expressions are used to **search for patterns** in text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "id": "266dff02",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0723-456-789\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "text = \"My number is 0723-456-789\"\n",
        "pattern = r\"\\d{4}-\\d{3}-\\d{3}\"\n",
        "match = re.search(pattern, text)\n",
        "print(match.group())  # 0723-456-789"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d12902fe",
      "metadata": {},
      "source": [
        "**Explanation**:\n",
        "\n",
        "* `\\d` → digit\n",
        "* `{n}` → match `n` times\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Feature Engineering for Text Data\n",
        "\n",
        "Before feeding text into models, we **clean and extract** useful features.\n",
        "\n",
        "Steps:\n",
        "\n",
        "1. Remove punctuation\n",
        "2. Lowercase\n",
        "3. Remove stopwords (like \"is\", \"the\", \"and\")\n",
        "4. Lemmatize (reduce to base word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "id": "49e2a14a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['cat', 'running', 'faster', 'dog']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\SAM\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\SAM\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "text = \"Cats are running faster than dogs\"\n",
        "words = word_tokenize(text.lower())\n",
        "\n",
        "words = [w for w in words if w not in stopwords.words('english')]\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = [lemmatizer.lemmatize(w) for w in words]\n",
        "\n",
        "print(words)  # ['cat', 'running', 'faster', 'dog']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98973135",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 6. Context-Free Grammars and POS Tagging\n",
        "\n",
        "### POS = Part of Speech\n",
        "\n",
        "We label each word with its role: noun, verb, adjective, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "id": "66445cb8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('The', 'DT'), ('dog', 'NN'), ('barks', 'VBZ'), ('loudly', 'RB'), ('.', '.')]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     d:\\DS_Lessons\\lessons\\phase-4\\Data\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "import os\n",
        "\n",
        "nltk_data_dir = os.path.abspath(r\"Data\\nltk_data\")\n",
        "nltk.data.path.insert(0, nltk_data_dir)\n",
        "\n",
        "# Download the new language-specific POS tagger\n",
        "nltk.download('averaged_perceptron_tagger_eng', download_dir=nltk_data_dir)\n",
        "\n",
        "text = word_tokenize(\"The dog barks loudly.\")\n",
        "tags = pos_tag(text)\n",
        "print(tags)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e115fba3",
      "metadata": {},
      "source": [
        "**Output**: `[('The', 'DT'), ('dog', 'NN'), ('barks', 'VBZ'), ('loudly', 'RB')]`\n",
        "\n",
        "* 'NN' = Noun\n",
        "* 'VBZ' = Verb\n",
        "* 'RB' = Adverb\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Text Classification\n",
        "\n",
        "### Task: Is this email spam?\n",
        "\n",
        "Steps:\n",
        "\n",
        "* Convert text to vectors (e.g., TF-IDF)\n",
        "* Train a classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "id": "7c811b4c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0 0]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "emails = [\"Free money\", \"Meeting today\", \"Win a car\", \"Project update\"]\n",
        "labels = [1, 0, 1, 0]  # 1 = spam, 0 = not spam\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(emails)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.5)\n",
        "\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(model.predict(X_test.toarray()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d592d315",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 8. Data Ethics in NLP\n",
        "\n",
        "### NLP Biases\n",
        "\n",
        "* If your dataset is biased (e.g., favoring one gender), your model will be too.\n",
        "* Offensive or harmful outputs can emerge from poor data curation.\n",
        "\n",
        "### Best Practices:\n",
        "\n",
        "* Use balanced and diverse datasets.\n",
        "* Audit model outputs for harmful language.\n",
        "* Always anonymize sensitive data.\n",
        "\n",
        "---\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. Tokenize the sentence: `\"Learning NLP is exciting!\"`\n",
        "2. Use TF-IDF on: `[\"AI is the future\", \"The future is now\"]`\n",
        "3. Create a regular expression to find email addresses.\n",
        "4. Classify the sentence \"Win a free trip!\" using Naive Bayes.\n",
        "5. Try POS tagging with NLTK on your own sentence.\n",
        "\n",
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "* NLP helps computers understand human language.\n",
        "* We use tools like TF-IDF, tokenization, and classification to analyze text.\n",
        "* Libraries like NLTK and scikit-learn make this easier.\n",
        "* Ethical awareness is just as important as accuracy.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
