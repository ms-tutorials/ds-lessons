{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "10f890ad",
      "metadata": {},
      "source": [
        "# Machine Learning Interpretability\n",
        "\n",
        "---\n",
        "\n",
        "## 1. What is Machine Learning Interpretability?\n",
        "\n",
        "Machine Learning Interpretability is **how well a human can understand why a model made a specific prediction**.\n",
        "\n",
        "### Anecdote:\n",
        "\n",
        "Imagine you're a judge. A computer gives you a decision: \"Do not grant bail.\" You’d want to know *why*. Was it the defendant’s record? Age? Past behavior? Interpretability helps you *understand the reasoning* behind machine predictions.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Why Is Interpretability Important?\n",
        "\n",
        "* **Trust**: Users trust models they can understand.\n",
        "* **Accountability**: Especially important in healthcare, finance, or law.\n",
        "* **Debugging**: Helps identify bias, errors, or wrong assumptions.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Two Types of Models by Interpretability\n",
        "\n",
        "| Model Type    | Example Algorithms                       | Interpretability  |\n",
        "| ------------- | ---------------------------------------- | ----------------- |\n",
        "| **White-Box** | Linear Regression, Decision Trees        | Easy to interpret |\n",
        "| **Black-Box** | Neural Networks, Random Forests, XGBoost | Hard to interpret |\n",
        "\n",
        "---\n",
        "\n",
        "## 4. White-Box Models\n",
        "\n",
        "### These are **transparent models** – you can see the \"decision process\".\n",
        "\n",
        "#### Example: Linear Regression\n",
        "\n",
        "**Model**:\n",
        "\n",
        "$$\n",
        "\\hat{y} = w_1x_1 + w_2x_2 + \\dots + w_nx_n + b\n",
        "$$\n",
        "\n",
        "### Python Code Example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9207ce69",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "import pandas as pd\n",
        "\n",
        "# Sample data\n",
        "df = pd.DataFrame({\n",
        "    'age': [25, 32, 47],\n",
        "    'salary': [50000, 60000, 75000]\n",
        "})\n",
        "y = [1, 0, 1]  # 1 = Will buy, 0 = Won’t buy\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(df, y)\n",
        "\n",
        "print(\"Coefficients:\", model.coef_)\n",
        "print(\"Intercept:\", model.intercept_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34fd52da",
      "metadata": {},
      "source": [
        "### Explanation:\n",
        "\n",
        "* `model.coef_` tells us how much each feature affects the outcome.\n",
        "* Positive = increases chance of prediction being 1.\n",
        "* Negative = decreases it.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Interpreting White-Boxes: Building a Decision Engine\n",
        "\n",
        "### Example: Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3868a54",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X = [[1], [2], [3], [10], [11], [12]]\n",
        "y = [0, 0, 0, 1, 1, 1]\n",
        "\n",
        "model = DecisionTreeClassifier(max_depth=2)\n",
        "model.fit(X, y)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plot_tree(model, filled=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff3da7c5",
      "metadata": {},
      "source": [
        "### What is this doing?\n",
        "\n",
        "* Splits data like a series of yes/no questions.\n",
        "* If value > 5.5 → Predict 1, else 0.\n",
        "\n",
        "#### Advantage:\n",
        "\n",
        "You can follow the path and understand **exactly why** a decision was made.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Black-Box Models\n",
        "\n",
        "These are models where we **can't easily explain how the decisions were made**.\n",
        "\n",
        "Examples: Neural Networks, Random Forests, Gradient Boosting, etc.\n",
        "\n",
        "### Why Use Them?\n",
        "\n",
        "* They perform **better on complex problems**, like image or voice recognition.\n",
        "* But they **sacrifice transparency**.\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Explaining Black-Boxes: Neural Networks\n",
        "\n",
        "Neural networks are made up of **layers of interconnected neurons**. Each layer transforms the data into something new. But this transformation is often **not human-readable**.\n",
        "\n",
        "### A Simplified Neural Network:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77feac80",
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(8, activation='relu', input_shape=(4,)),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "568ab724",
      "metadata": {},
      "source": [
        "This model:\n",
        "\n",
        "* Takes 4 input features.\n",
        "* Processes them through a hidden layer.\n",
        "* Produces a binary output (0 or 1).\n",
        "\n",
        "---\n",
        "\n",
        "## How Do We Interpret Black-Boxes?\n",
        "\n",
        "### Tools:\n",
        "\n",
        "* **SHAP** (SHapley Additive exPlanations)\n",
        "* **LIME** (Local Interpretable Model-agnostic Explanations)\n",
        "* **Permutation Importance**\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Interpreting with SHAP (Example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9aeb2142",
      "metadata": {},
      "outputs": [],
      "source": [
        "import shap\n",
        "import xgboost\n",
        "from sklearn.datasets import load_boston\n",
        "\n",
        "# Load data\n",
        "X, y = load_boston(return_X_y=True)\n",
        "model = xgboost.XGBRegressor().fit(X, y)\n",
        "\n",
        "# Explain predictions\n",
        "explainer = shap.Explainer(model)\n",
        "shap_values = explainer(X)\n",
        "\n",
        "# Visualize\n",
        "shap.plots.waterfall(shap_values[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fd14227",
      "metadata": {},
      "source": [
        "### Explanation:\n",
        "\n",
        "* This tells you how each feature **contributed to a single prediction**.\n",
        "* Blue = pushed prediction lower\n",
        "* Red = pushed prediction higher\n",
        "\n",
        "---\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. **White Box**: Train a DecisionTreeClassifier and explain its decisions using `plot_tree()`.\n",
        "2. **Black Box**: Train a neural network or XGBoost model. Use SHAP to interpret one prediction.\n",
        "3. **Compare**: Train a white-box and a black-box model on the same dataset. Compare their performance and interpretability.\n",
        "4. What are some real-world examples where a white-box model would be *more important* than accuracy?\n",
        "\n",
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "* **Interpretability** is key in building trust in ML models.\n",
        "* **White-box models** like decision trees and linear regression are easy to understand.\n",
        "* **Black-box models** are powerful but need tools like SHAP or LIME to explain.\n",
        "* **Interpretation** is essential in healthcare, finance, law, and other sensitive domains.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "# Machine Learning Interpretability – Black Box Models\n",
        "\n",
        "## Explaining Black Boxes: Neural Networks with SHAP and LIME\n",
        "\n",
        "---\n",
        "\n",
        "## Introduction\n",
        "\n",
        "As machine learning becomes more powerful, especially with complex models like **neural networks**, **XGBoost**, and **ensemble models**, it also becomes harder to **understand** why these models make the decisions they do.\n",
        "\n",
        "This is often referred to as the **“black-box” problem** - we get an answer (a prediction), but we don't always know why.\n",
        "\n",
        "To tackle this, we use tools like **SHAP** (SHapley Additive exPlanations) and **LIME** (Local Interpretable Model-Agnostic Explanations) to **interpret** these models.\n",
        "\n",
        "---\n",
        "\n",
        "## Why Interpretability Matters\n",
        "\n",
        "Let’s say a neural network denies someone a loan.\n",
        "\n",
        "* Would you trust it without knowing why?\n",
        "* Could you appeal the decision?\n",
        "* Could a doctor trust an AI diagnosis without understanding the reason?\n",
        "\n",
        "**Interpretability bridges the gap between predictions and trust.**\n",
        "\n",
        "---\n",
        "\n",
        "## What is a Black Box Model?\n",
        "\n",
        "A black box model is a model whose **internal logic is hidden** or **too complex** to explain easily.\n",
        "\n",
        "Examples:\n",
        "\n",
        "* Deep Neural Networks\n",
        "* Gradient Boosted Trees (e.g., XGBoost)\n",
        "* Random Forests\n",
        "\n",
        "---\n",
        "\n",
        "## Solution: Model-Agnostic Explanations\n",
        "\n",
        "We use model-agnostic techniques like **SHAP** and **LIME**, which work **outside** the model. Think of them like asking:\n",
        "\n",
        "> “What happens to the output when we tweak the input just a little?”\n",
        "\n",
        "---\n",
        "\n",
        "## LIME: Local Interpretable Model-agnostic Explanations\n",
        "\n",
        "### How LIME works\n",
        "\n",
        "* Pick a specific prediction (e.g., why this person was predicted “No Loan”).\n",
        "* Generate small variations of the input (perturbations).\n",
        "* Observe how predictions change.\n",
        "* Fit a simple, interpretable model (like a linear model) around that local point.\n",
        "\n",
        "### Code Example: Explaining a Classifier Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d80e07b",
      "metadata": {},
      "outputs": [],
      "source": [
        "import lime\n",
        "import lime.lime_tabular\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load data\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "\n",
        "# Train model\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Create explainer\n",
        "explainer = lime.lime_tabular.LimeTabularExplainer(\n",
        "    training_data=X_train,\n",
        "    feature_names=['sepal length', 'sepal width', 'petal length', 'petal width'],\n",
        "    class_names=['setosa', 'versicolor', 'virginica'],\n",
        "    mode='classification'\n",
        ")\n",
        "\n",
        "# Pick a sample to explain\n",
        "i = 1\n",
        "exp = explainer.explain_instance(X_test[i], model.predict_proba)\n",
        "exp.show_in_notebook()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3871db3",
      "metadata": {},
      "source": [
        "### What LIME Shows\n",
        "\n",
        "You’ll see a **bar chart** showing:\n",
        "\n",
        "* Which features pushed the prediction **up**\n",
        "* Which features pulled the prediction **down**\n",
        "\n",
        "---\n",
        "\n",
        "## SHAP: SHapley Additive ExPlanations\n",
        "\n",
        "### What SHAP Does\n",
        "\n",
        "SHAP is based on **game theory**. Imagine each feature is a player in a game, and the prediction is the **payout**. SHAP asks:\n",
        "\n",
        "> “How much does each player (feature) contribute to the final payout (prediction)?”\n",
        "\n",
        "### Key Idea\n",
        "\n",
        "SHAP values are **additive**:\n",
        "The prediction = base value + sum of SHAP values\n",
        "\n",
        "### Code Example with Tree Models (e.g., XGBoost, Random Forest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a68bdff7",
      "metadata": {},
      "outputs": [],
      "source": [
        "import shap\n",
        "\n",
        "# Load SHAP explainer for tree-based models\n",
        "explainer = shap.Explainer(model, X_train)\n",
        "\n",
        "# Pick a sample to explain\n",
        "shap_values = explainer(X_test)\n",
        "\n",
        "# Visualize\n",
        "shap.plots.waterfall(shap_values[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90b6bf1d",
      "metadata": {},
      "source": [
        "### Other Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e80ac09",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary plot for feature importance\n",
        "shap.plots.beeswarm(shap_values)\n",
        "\n",
        "# Dependence plot (effect of one feature)\n",
        "shap.plots.scatter(shap_values[:, \"petal length\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7df93db6",
      "metadata": {},
      "source": [
        "### What SHAP Shows\n",
        "\n",
        "* Waterfall Plot: Step-by-step contribution of each feature to final prediction.\n",
        "* Beeswarm Plot: Overall feature importance for the dataset.\n",
        "* Dependence Plot: How one feature impacts predictions across examples.\n",
        "\n",
        "---\n",
        "\n",
        "## SHAP vs LIME: When to Use Which?\n",
        "\n",
        "| Feature          | SHAP                       | LIME                                  |\n",
        "| ---------------- | -------------------------- | ------------------------------------- |\n",
        "| Method           | Global + Local             | Local only                            |\n",
        "| Math Basis       | Game Theory                | Local regression                      |\n",
        "| Model Support    | Tree, NN, Linear, etc.     | All models (model-agnostic)           |\n",
        "| Speed            | Slower                     | Faster                                |\n",
        "| Interpretability | Very accurate & consistent | Easier to explain but sometimes noisy |\n",
        "\n",
        "---\n",
        "\n",
        "## Example: Explaining a Neural Network\n",
        "\n",
        "Let’s use SHAP to explain a neural network trained with Keras:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b97b323b",
      "metadata": {},
      "outputs": [],
      "source": [
        "import shap\n",
        "import tensorflow as tf\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load data\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "\n",
        "# Define a simple neural network\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(10, activation='relu', input_shape=(4,)),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=10, verbose=0)\n",
        "\n",
        "# Explain using SHAP\n",
        "explainer = shap.Explainer(model, X_train)\n",
        "shap_values = explainer(X_test)\n",
        "\n",
        "# Plot\n",
        "shap.plots.waterfall(shap_values[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8726c6d",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Real-World Analogy\n",
        "\n",
        "Imagine a restaurant bill split among friends:\n",
        "\n",
        "* SHAP helps you know **exactly how much each friend** contributed to the bill.\n",
        "* LIME is like interviewing your friends **near one table** to see who likely paid more.\n",
        "\n",
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "* Black-box models are powerful but hard to interpret.\n",
        "* **LIME** helps explain individual predictions locally.\n",
        "* **SHAP** gives fair, consistent attributions globally and locally.\n",
        "* Both tools build **trust** and **transparency** in machine learning.\n",
        "\n",
        "---\n",
        "\n",
        "## Mini Exercises\n",
        "\n",
        "1. Use `RandomForestClassifier` on a dataset and use **LIME** to explain one prediction.\n",
        "2. Use `shap` to generate a **beeswarm plot** for a tree model.\n",
        "3. Which features are **consistently important** across examples?\n",
        "4. Try using SHAP to explain a neural network trained with Keras.\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
